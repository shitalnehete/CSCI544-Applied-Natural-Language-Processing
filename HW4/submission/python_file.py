# -*- coding: utf-8 -*-
"""CSCI544-HW4 main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ObtBG7KMNyPw2kT_0LxmfIsQmeJt6diJ
"""

"""# Importing libraries"""

import numpy as np
import pandas as pd
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import StepLR
import random
import json
torch.manual_seed(0)
random.seed(0)

import sys

train_file = sys.argv[1]
dev_file = sys.argv[2]
test_file = sys.argv[3]
glove_file = sys.argv[4]

print(train_file, dev_file, test_file, glove_file)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

"""# Preparing Data"""

df_train = list()
with open(train_file, 'r') as f:
    for line in f.readlines():
        if len(line) > 1:
            id, word, ner= line.strip().split(" ")
            df_train.append([id, word, ner])

df_train = pd.DataFrame(df_train, columns=['id', 'word', 'NER'])
df_train = df_train.dropna()

train_x, train_y = [], []
x, y = [], []
first=1

for row in df_train.itertuples():
    if(row.id == '1' and first == 0):
        train_x.append(x)
        train_y.append(y)
        x=[]
        y=[]
    first=0
    x.append(row.word)
    y.append(row.NER)

df_dev = list()
with open(dev_file, 'r') as f:
    for line in f.readlines():
        if len(line) > 1:
            id, word, ner = line.strip().split(" ")
            df_dev.append([id, word, ner])

df_dev = pd.DataFrame(df_dev, columns=['id', 'word', 'NER'])
df_dev = df_dev.dropna()

dev_x, dev_y = [], []
x, y = [], []
first=1

for row in df_dev.itertuples():
    if(row.id == '1' and first == 0):
        dev_x.append(x)
        dev_y.append(y)
        x=[]
        y=[]
    first=0
    x.append(row.word)
    y.append(row.NER)

df_test = list()
with open(test_file, 'r') as f:
    for line in f.readlines():
        if len(line) > 1:
            id, word = line.strip().split(" ")
            df_test.append([id, word])

df_test = pd.DataFrame(df_test, columns=['id', 'word'])
df_test = df_test.dropna()

test_x = []
x = []
first=1

for row in df_test.itertuples():
    if(row.id == '1' and first == 0):
        test_x.append(x)
        x=[]
    first=0
    x.append(row.word)

"""# Creating vocabulary and labels"""

index=1
word2idx={'<pad>': 0}
rev_vocab_dict = {0: '<pad>'}
for x in [train_x, dev_x, test_x]:
    for sentence in x:
        for word in sentence:
            if word not in word2idx:
                word2idx[word] = index
                rev_vocab_dict[index] = word
                index+=1

# len(word2idx)

# with open('/content/drive/My Drive/Colab Notebooks/HW4-CSCI544/word2idx.json', 'w') as f:
#     json.dump(word2idx, f)

labels = set()
label_dict = {}
rev_label_dict = {}
index=0

for x in [train_y, dev_y]:
    for sentence in x:
        for label in sentence:
            labels.add(label)
            if label not in label_dict:
                label_dict[label] = index
                rev_label_dict[index] = label
                index+=1

# label_dict

# with open('/content/drive/My Drive/Colab Notebooks/HW4-CSCI544/label_dict.json', 'w') as f:
#     json.dump(label_dict, f)

"""# Vectorizing sentences and labels"""

train_x_vec = []
x = []

for words in train_x:
    for word in words:
        x.append(word2idx[word])
    train_x_vec.append(x)
    x = []

dev_x_vec = []
x = []

for words in dev_x:
    for word in words:
        x.append(word2idx[word])
    dev_x_vec.append(x)
    x = []

test_x_vec = []
x = []

for words in test_x:
    for word in words:
        x.append(word2idx[word])
    test_x_vec.append(x)
    x = []

train_y_vec = []

for tags in train_y:
    y = []
    for label in tags:
        y.append(label_dict[label])
    train_y_vec.append(y)

dev_y_vec = []

for tags in dev_y:
    y = []
    for label in tags:
        y.append(label_dict[label])
    dev_y_vec.append(y)

"""# Bidirectional LSTM """

class BiLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, linear_out_dim, hidden_dim, lstm_layers, bidirectional, dropout_val, tag_size, glove_flag, emb_matrix):
        super(BiLSTM, self).__init__()
        """ Hyper Parameters """
        self.hidden_dim = hidden_dim  # hidden_dim = 256
        self.lstm_layers = lstm_layers  # LSTM Layers = 1
        # self.embedding_dim = embedding_dim  # Embedding Dimension = 100
        # self.linear_out_dim = linear_out_dim  # Linear Ouput Dimension = 128
        # self.tag_size = tag_size  # Tag Size = 9
        self.num_directions = 2 if bidirectional else 1

        """ Initializing Network """
        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embedding Layer
        
        if(glove_flag): self.embedding.weight = nn.Parameter(torch.tensor(emb_matrix))
        else: self.embedding.weight.data.uniform_(-1,1)

        self.LSTM = nn.LSTM(embedding_dim,
                            hidden_dim,
                            num_layers=lstm_layers,
                            batch_first=True,
                            bidirectional=True)
        self.fc = nn.Linear(hidden_dim*self.num_directions, linear_out_dim)  # 2 for bidirection
        self.dropout = nn.Dropout(dropout_val)
        self.elu = nn.ELU(alpha=0.01)
        self.classifier = nn.Linear(linear_out_dim, tag_size)


    def init_hidden(self, batch_size):
        h, c = (torch.zeros(self.lstm_layers * self.num_directions, batch_size, self.hidden_dim).to(device),
                torch.zeros(self.lstm_layers * self.num_directions, batch_size, self.hidden_dim).to(device))
        return h, c

    def forward(self, sen, sen_len):  # sen_len
        # Set initial states
        batch_size = sen.shape[0]
        h_0, c_0 = self.init_hidden(batch_size)

        # Forward propagate LSTM
        embedded = self.embedding(sen).float()
        packed_embedded = pack_padded_sequence(embedded, sen_len, batch_first=True, enforce_sorted=False)
        output, _ = self.LSTM(packed_embedded, (h_0, c_0))
        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)
        dropout = self.dropout(output_unpacked)
        lin = self.fc(dropout)
        pred = self.elu(lin)
        pred = self.classifier(pred)
        return pred

class BiLSTM_DataLoader(Dataset):
    def __init__(self, x, y):
        self.x = x
        self.y = y

    def __len__(self):
        return len(self.x)

    def __getitem__(self, index):
        x_instance = torch.tensor(self.x[index])  # , dtype=torch.long
        y_instance = torch.tensor(self.y[index])  # , dtype=torch.float
        return x_instance, y_instance

class CustomCollator(object):

    def __init__(self, vocab, label):
        self.params = vocab
        self.label = label

    def __call__(self, batch):
        (xx, yy) = zip(*batch)
        x_len = [len(x) for x in xx]
        y_len = [len(y) for y in yy]
        batch_max_len = max([len(s) for s in xx])
        batch_data = self.params['<pad>']*np.ones((len(xx), batch_max_len))
        batch_labels = -1*np.zeros((len(xx), batch_max_len))
        for j in range(len(xx)):
            cur_len = len(xx[j])
            batch_data[j][:cur_len] = xx[j]
            batch_labels[j][:cur_len] = yy[j]

        batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)
        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)

        return batch_data, batch_labels, x_len, y_len

class_weights = dict()
for key in label_dict:
    class_weights[key] = 0
total_nm_tags = 0
for data in [train_y, dev_y]:
    for tags in data:
        for tag in tags:
            total_nm_tags += 1
            class_weights[tag] += 1

class_wt = list()
for key in class_weights.keys():
    if class_weights[key]:
        score = round(math.log(0.35*total_nm_tags / class_weights[key]), 2)
        class_weights[key] = score if score > 1.0 else 1.0
    else:
        class_weights[key] = 1.0
    class_wt.append(class_weights[key])
class_wt = torch.tensor(class_wt)

"""## Hyperparameters:
### Embedding dimension = 100
### Hidden dimension = 256
### Linear Output dimension = 128
### Bidirectional = True
### Dropout = 0.33
### Number of LSTM layers = 1
### Batch Size = 4
### Loss Function = Cross Entropy with class weights
### Optimizer = SGD with Learning Rate = 0.1 and Momentum = 0.9
### Epochs = 200
"""

# BiLSTM_model = BiLSTM(vocab_size=len(word2idx),
#                       embedding_dim=100,
#                       linear_out_dim=128,
#                       hidden_dim=256,
#                       lstm_layers=1,
#                       bidirectional=True,
#                       dropout_val=0.33,
#                       tag_size=len(label_dict),
#                       glove_flag=False,
#                       emb_matrix=[])

# BiLSTM_model.to(device)
# print(BiLSTM_model)

# BiLSTM_train = BiLSTM_DataLoader(train_x_vec, train_y_vec)
# custom_collator = CustomCollator(word2idx, label_dict)
# dataloader = DataLoader(dataset=BiLSTM_train,
#                         batch_size=4,
#                         drop_last=True,
#                         collate_fn=custom_collator)

# criterion = nn.CrossEntropyLoss(weight=class_wt)

# criterion = criterion.to(device)
# criterion.requres_grad = True
# optimizer = torch.optim.SGD(BiLSTM_model.parameters(), lr=0.1, momentum=0.9)
# epochs = 200

# for i in range(1, epochs+1):
#     train_loss = 0.0
    
#     for input, label, input_len, label_len in dataloader:
#         optimizer.zero_grad()
#         output = BiLSTM_model(input.to(device), input_len)  # input_len
#         output = output.view(-1, len(label_dict))
#         label = label.view(-1)
#         loss = criterion(output, label.to(device))
         
#         loss.backward()
#         optimizer.step()
#         train_loss += loss.item() * input.size(1)

#     train_loss = train_loss / len(dataloader.dataset)
#     print('Epoch: {} \tTraining Loss: {:.6f}'.format(i, train_loss))
#     torch.save(BiLSTM_model.state_dict(), '/content/drive/My Drive/Colab Notebooks/HW4-CSCI544/BiLSTM/BiLSTM_epoch_' + str(i) + '.pt')
# torch.save(BiLSTM_model.state_dict(), '/content/drive/My Drive/Colab Notebooks/HW4-CSCI544/BiLSTM/blstm1.pt')

BiLSTM_model = BiLSTM(vocab_size=len(word2idx),
                      embedding_dim=100,
                      linear_out_dim=128,
                      hidden_dim=256,
                      lstm_layers=1,
                      bidirectional=True,
                      dropout_val=0.33,
                      tag_size=len(label_dict),
                      glove_flag=False,
                      emb_matrix=[])

BiLSTM_model.load_state_dict(torch.load("blstm1.pt"))
BiLSTM_model.to(device)

# #tesing on validation data
# BiLSTM_dev = BiLSTM_DataLoader(dev_x_vec, dev_y_vec)
# custom_collator = CustomCollator(word2idx, label_dict)
# dataloader_dev = DataLoader(dataset=BiLSTM_dev,
#                             batch_size=4,
#                             shuffle=False,
#                             drop_last=True,
#                             collate_fn=custom_collator)


# file = open("/content/drive/My Drive/Colab Notebooks/HW4-CSCI544/dev1_train.out", 'w')
# for dev_data, label, dev_data_len, label_data_len in dataloader_dev:

#     pred = BiLSTM_model(dev_data.to(device), dev_data_len)
#     pred = pred.cpu()
#     pred = pred.detach().numpy()
#     label = label.detach().numpy()
#     dev_data = dev_data.detach().numpy()
#     pred = np.argmax(pred, axis=2)
#     pred = pred.reshape((len(label), -1))

#     for i in range(len(dev_data)):
#         for j in range(len(dev_data[i])):
#             if dev_data[i][j] != 0:
#                 word = rev_vocab_dict[dev_data[i][j]]
#                 gold = rev_label_dict[label[i][j]]
#                 op = rev_label_dict[pred[i][j]]
#                 file.write(" ".join([str(j+1), str(word), gold, op]))
#                 file.write("\n")
#         file.write("\n")

# file.close()

# !perl '/content/drive/My Drive/Colab Notebooks/HW4-CSCI544/conll03eval.txt' < '/content/drive/My Drive/Colab Notebooks/HW4-CSCI544/dev1_train.out'

"""## What are the precision, recall and F1 score on the dev data? 

### accuracy:  95.36%
### precision:  78.60%
### recall:  74.80%
### FB1:  76.65
"""

#tesing on validation data
BiLSTM_dev = BiLSTM_DataLoader(dev_x_vec, dev_y_vec)
custom_collator = CustomCollator(word2idx, label_dict)
dataloader_dev = DataLoader(dataset=BiLSTM_dev,
                            batch_size=4,
                            shuffle=False,
                            drop_last=True,
                            collate_fn=custom_collator)


file = open("dev1.out", 'w')
for dev_data, label, dev_data_len, label_data_len in dataloader_dev:

    pred = BiLSTM_model(dev_data.to(device), dev_data_len)
    pred = pred.cpu()
    pred = pred.detach().numpy()
    label = label.detach().numpy()
    dev_data = dev_data.detach().numpy()
    pred = np.argmax(pred, axis=2)
    pred = pred.reshape((len(label), -1))

    for i in range(len(dev_data)):
        for j in range(len(dev_data[i])):
            if dev_data[i][j] != 0:
                word = rev_vocab_dict[dev_data[i][j]]
                # gold = rev_label_dict[label[i][j]]
                op = rev_label_dict[pred[i][j]]
                file.write(" ".join([str(j+1), str(word), op]))
                file.write("\n")
        file.write("\n")

file.close()

class BiLSTM_TestLoader(Dataset):
    def __init__(self, x):
        self.x = x

    def __len__(self):
        return len(self.x)

    def __getitem__(self, index):
        x_instance = torch.tensor(self.x[index])  # , dtype=torch.long
        # y_instance = torch.tensor(self.y[index])  # , dtype=torch.float
        return x_instance
        

class CustomTestCollator(object):

    def __init__(self, vocab, label):
        self.params = vocab
        self.label = label

    def __call__(self, batch):
        xx = batch
        x_len = [len(x) for x in xx]
        # y_len = [len(y) for y in yy]
        batch_max_len = max([len(s) for s in xx])
        batch_data = self.params['<pad>']*np.ones((len(xx), batch_max_len))
        # batch_labels = -1*np.zeros((len(xx), batch_max_len))
        for j in range(len(xx)):
            cur_len = len(xx[j])
            batch_data[j][:cur_len] = xx[j]
            # batch_labels[j][:cur_len] = yy[j]

        batch_data = torch.LongTensor(batch_data)
        batch_data = Variable(batch_data)

        return batch_data, x_len

#Testing on Testing Dataset 

BiLSTM_test = BiLSTM_TestLoader(test_x_vec)
custom_test_collator = CustomTestCollator(word2idx, label_dict)
dataloader_test = DataLoader(dataset=BiLSTM_test,
                                batch_size=4,
                                shuffle=False,
                                drop_last=True,
                                collate_fn=custom_test_collator)


file = open("test1.out", 'w')
for test_data, test_data_len in dataloader_test:

    pred = BiLSTM_model(test_data.to(device), test_data_len)
    pred = pred.cpu()
    pred = pred.detach().numpy()
    test_data = test_data.detach().numpy()
    pred = np.argmax(pred, axis=2)
    pred = pred.reshape((len(test_data), -1))
    
    for i in range(len(test_data)):
        for j in range(len(test_data[i])):
            if test_data[i][j] != 0:
                word = rev_vocab_dict[test_data[i][j]]
                op = rev_label_dict[pred[i][j]]
                file.write(" ".join([str(j+1), word, op]))
                file.write("\n")

        file.write("\n")
        
file.close()

"""# GloVe Word Embeddings

### The function "create_emb_matrix" generates a matrix using the dictionary of the glove model, which is then used to input data into the Bilstm model. It is important to note that the glove model dictionary only contains lowercase words, and to address this issue, we adjust the embedding for titled words by adding a slight displacement value to each dimension of the lowercase counterpart.
"""

def create_emb_matrix(word_idx, emb_dict, dimension):

    emb_matrix = np.zeros((len(word_idx), dimension))
    for word, idx in word_idx.items():
        if word in emb_dict:
            emb_matrix[idx] = emb_dict[word]
        else:
            if word.lower() in emb_dict:
                emb_matrix[idx] = emb_dict[word.lower()] + 5e-3
            else:
                pass
    return emb_matrix

glove = pd.read_csv(glove_file, sep=" ",quoting=3, header=None, index_col=0)
glove_emb = {key: val.values for key, val in glove.T.items()}
glove_vec = np.array([glove_emb[key] for key in glove_emb])
glove_emb["<pad>"] = np.zeros((100,), dtype="float64")
emb_matrix = create_emb_matrix(word_idx=word2idx, emb_dict=glove_emb, dimension=100)

vocab_size = emb_matrix.shape[0]
vector_size = emb_matrix.shape[1]
print(vocab_size, vector_size)

"""## Hyperparameters:
### Embedding dimension = 100
### Hidden dimension = 256
### Linear Output dimension = 128
### Bidirectional = True
### Dropout = 0.33
### Number of LSTM layers = 1
### Batch Size = 8
### Loss Function = Cross Entropy with class weights
### Optimizer = SGD with Learning Rate = 0.1 and Momentum = 0.9
### Epochs = 50
"""

# BiLSTM_glove_model = BiLSTM(vocab_size=len(word2idx),
#                       embedding_dim=100,
#                       linear_out_dim=128,
#                       hidden_dim=256,
#                       lstm_layers=1,
#                       bidirectional=True,
#                       dropout_val=0.33,
#                       tag_size=len(label_dict),
#                       glove_flag=True,
#                       emb_matrix=emb_matrix)

# BiLSTM_glove_model.to(device)
# print(BiLSTM_glove_model)

# BiLSTM_train = BiLSTM_DataLoader(train_x_vec, train_y_vec)
# custom_collator = CustomCollator(word2idx, label_dict)
# dataloader = DataLoader(dataset=BiLSTM_train,
#                         batch_size=8,
#                         drop_last=True,
#                         collate_fn=custom_collator)

# criterion = nn.CrossEntropyLoss(weight=class_wt)

# criterion = criterion.to(device)
# criterion.requres_grad = True
# optimizer = torch.optim.SGD(BiLSTM_glove_model.parameters(), lr=0.1, momentum=0.9)

# scheduler = StepLR(optimizer, step_size=15, gamma=0.9)
# epochs = 50

# for i in range(1, epochs+1):
#     train_loss = 0.0
    
#     for input, label, input_len, label_len in dataloader:
#         optimizer.zero_grad()
        
#         output = BiLSTM_glove_model(input.to(device), input_len)  # input_len
#         output = output.view(-1, len(label_dict))
#         label = label.view(-1)
#         loss = criterion(output, label.to(device))
        
#         loss.backward()
#         optimizer.step()
#         train_loss += loss.item() * input.size(1)

#     train_loss = train_loss / len(dataloader.dataset)
#     print('Epoch: {} \tTraining Loss: {:.6f}'.format(i, train_loss))
#     torch.save(BiLSTM_glove_model.state_dict(), '/content/drive/My Drive/Colab Notebooks/HW4-CSCI544/BiLSTM_glove/BiLSTM_glove_' + str(i) + '.pt')
# torch.save(BiLSTM_glove_model.state_dict(), '/content/drive/My Drive/Colab Notebooks/HW4-CSCI544/BiLSTM_glove/blstm2.pt')

BiLSTM_glove_model = BiLSTM(vocab_size=len(word2idx),
                        embedding_dim=100,
                        linear_out_dim=128,
                        hidden_dim=256,
                        lstm_layers=1,
                        bidirectional = True,
                        dropout_val=0.33,
                        tag_size=len(label_dict),
                        glove_flag = True,
                        emb_matrix=emb_matrix)

BiLSTM_glove_model.load_state_dict(torch.load("blstm2.pt"))
BiLSTM_glove_model.to(device)

# #predicting for validation dataset
# BiLSTM_dev = BiLSTM_DataLoader(dev_x_vec, dev_y_vec)
# custom_collator = CustomCollator(word2idx, label_dict)
# dataloader_dev = DataLoader(dataset=BiLSTM_dev,
#                             batch_size=8,
#                             shuffle=False,
#                             drop_last=True,
#                             collate_fn=custom_collator)
# print(label_dict)

# res = []
# file = open("/content/drive/My Drive/Colab Notebooks/HW4-CSCI544/dev2_train.out", 'w')
# for dev_data, label, dev_data_len, label_data_len in dataloader_dev:

#     pred = BiLSTM_glove_model(dev_data.to(device), dev_data_len)
#     pred = pred.cpu()
#     pred = pred.detach().numpy()
#     label = label.detach().numpy()
#     dev_data = dev_data.detach().numpy()
#     pred = np.argmax(pred, axis=2)
#     pred = pred.reshape((len(label), -1))

#     for i in range(len(dev_data)):
#         for j in range(len(dev_data[i])):
#             if dev_data[i][j] != 0:
#                 word = rev_vocab_dict[dev_data[i][j]]
#                 gold = rev_label_dict[label[i][j]]
#                 op = rev_label_dict[pred[i][j]]
#                 res.append((word, gold, op))
#                 file.write(" ".join([str(j + 1), str(word), gold, op]))
#                 file.write("\n")
#         file.write("\n")
# file.close()

# !perl '/content/drive/My Drive/Colab Notebooks/HW4-CSCI544/conll03eval.txt' < '/content/drive/My Drive/Colab Notebooks/HW4-CSCI544/dev2_train.out'

"""## What are the precision, recall and F1 score on the dev data? 
### accuracy:  98.02%
### precision:  89.23%
### recall:  90.12%
### FB1:  89.67

"""

#predicting for validation dataset
BiLSTM_dev = BiLSTM_DataLoader(dev_x_vec, dev_y_vec)
custom_collator = CustomCollator(word2idx, label_dict)
dataloader_dev = DataLoader(dataset=BiLSTM_dev,
                            batch_size=8,
                            shuffle=False,
                            drop_last=True,
                            collate_fn=custom_collator)
print(label_dict)

res = []
file = open("dev2.out", 'w')
for dev_data, label, dev_data_len, label_data_len in dataloader_dev:

    pred = BiLSTM_glove_model(dev_data.to(device), dev_data_len)
    pred = pred.cpu()
    pred = pred.detach().numpy()
    label = label.detach().numpy()
    dev_data = dev_data.detach().numpy()
    pred = np.argmax(pred, axis=2)
    pred = pred.reshape((len(label), -1))

    for i in range(len(dev_data)):
        for j in range(len(dev_data[i])):
            if dev_data[i][j] != 0:
                word = rev_vocab_dict[dev_data[i][j]]
                # gold = rev_label_dict[label[i][j]]
                op = rev_label_dict[pred[i][j]]
                file.write(" ".join([str(j + 1), str(word), op]))
                file.write("\n")
        file.write("\n")
file.close()

#predicting for testing dataset
BiLSTM_test = BiLSTM_TestLoader(test_x_vec)
custom_test_collator = CustomTestCollator(word2idx, label_dict)
dataloader_test = DataLoader(dataset=BiLSTM_test,
                                batch_size=1,
                                shuffle=False,
                                drop_last=True,
                                collate_fn=custom_test_collator)

res = []
file = open("test2.out", 'w')
for test_data, test_data_len in dataloader_test:

    pred = BiLSTM_glove_model(test_data.to(device), test_data_len)
    pred = pred.cpu()
    pred = pred.detach().numpy()
    # label = label.detach().numpy()
    test_data = test_data.detach().numpy()
    pred = np.argmax(pred, axis=2)
    pred = pred.reshape((len(test_data), -1))

    for i in range(len(test_data)):
        for j in range(len(test_data[i])):
            if test_data[i][j] != 0:
                word = rev_vocab_dict[test_data[i][j]]
                # gold = rev_label_dict[label[i][j]]
                op = rev_label_dict[pred[i][j]]
                res.append((word, op))
                file.write(" ".join([str(j + 1), word, op]))
                file.write("\n")
        file.write("\n")
file.close()

emb_matrix

