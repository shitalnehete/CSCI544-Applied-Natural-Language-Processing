{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versions\n",
    "python 3.10.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YwQ1y-DekJL"
   },
   "source": [
    "# Task 1: Data Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 777,
     "status": "ok",
     "timestamp": 1677377511729,
     "user": {
      "displayName": "Shital Virendra Nehete",
      "userId": "02901240228711503219"
     },
     "user_tz": 480
    },
    "id": "4BOxXuVRDOJp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd #'1.5.2'\n",
    "import numpy as np #'1.23.5'\n",
    "import re #'2.2.1'\n",
    "\n",
    "import gensim #'4.3.0'\n",
    "import gensim.downloader as api\n",
    "\n",
    "from numpy import argmax\n",
    "from numpy import vstack\n",
    "\n",
    "import sklearn #'1.2.1'\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch #'1.13.1'\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset generation and saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1344,
     "status": "ok",
     "timestamp": 1677377513312,
     "user": {
      "displayName": "Shital Virendra Nehete",
      "userId": "02901240228711503219"
     },
     "user_tz": 480
    },
    "id": "kNc1PINbEm2X",
    "outputId": "c59576e0-9014-4f5d-b503-ed60f8c43794"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_j/sb0mgtc11kg8cclmrq736qbr0000gn/T/ipykernel_13664/319090821.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv('data.tsv', sep='\\t', usecols=['review_body', 'star_rating'])\n"
     ]
    }
   ],
   "source": [
    "# df=pd.read_csv('data.csv', usecols=['review_body', 'rating_class'])\n",
    "df=pd.read_csv('data.tsv', sep='\\t', usecols=['review_body', 'star_rating'])\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1677377511730,
     "user": {
      "displayName": "Shital Virendra Nehete",
      "userId": "02901240228711503219"
     },
     "user_tz": 480
    },
    "id": "11hiEKMQDQ2Q"
   },
   "outputs": [],
   "source": [
    "#get 20k reviews for each class\n",
    "df1 = df.query(\"star_rating == '1' | star_rating == '2'\").sample(n=20000, random_state=65)\n",
    "df1 = df1.assign(rating_class=0)\n",
    "\n",
    "df2 = df.query(\"star_rating == '3'\").sample(n=20000, random_state=65)\n",
    "df2 = df2.assign(rating_class=1)\n",
    "\n",
    "df3 = df.query(\"star_rating == '4' | star_rating == '5'\").sample(n=20000, random_state=65)\n",
    "df3 = df3.assign(rating_class=2)\n",
    "\n",
    "df = pd.concat([df1[['review_body', 'rating_class']], df2[['review_body', 'rating_class']], df3[['review_body', 'rating_class']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1677377511971,
     "user": {
      "displayName": "Shital Virendra Nehete",
      "userId": "02901240228711503219"
     },
     "user_tz": 480
    },
    "id": "3_9agUvTD7ZQ"
   },
   "outputs": [],
   "source": [
    "# path = 'data.csv'\n",
    "# with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
    "#   final_dataset.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 2644,
     "status": "ok",
     "timestamp": 1677377515952,
     "user": {
      "displayName": "Shital Virendra Nehete",
      "userId": "02901240228711503219"
     },
     "user_tz": 480
    },
    "id": "6A_AngCWHeZG",
    "outputId": "b746eae9-3aa6-4059-e0db-05af311e1698"
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "  try:    \n",
    "    string = re.sub(r'^https?:\\/\\/<>.*[\\r\\n]*', '', string, flags=re.MULTILINE)\n",
    "    string = re.sub(r\"[^A-Za-z]\", \" \", string)         \n",
    "    words = string.strip().lower().split()    \n",
    "    words = [w for w in words if len(w)>=1]\n",
    "    return \" \".join(words)\t\n",
    "  except:\n",
    "    return \"\"\n",
    "\n",
    "#Clean the Data using routine above\n",
    "\n",
    "df['clean_review_body'] = df['review_body'].apply(clean_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'processed_data.csv'\n",
    "# with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
    "#   df.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujv_lmE5eqOL"
   },
   "source": [
    "# Task 2: Word Embedding (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 2006,
     "status": "ok",
     "timestamp": 1677377517953,
     "user": {
      "displayName": "Shital Virendra Nehete",
      "userId": "02901240228711503219"
     },
     "user_tz": 480
    },
    "id": "8m8zTmddGZjk"
   },
   "source": [
    "### Pretrained Word Embedding model (Google News Word2Vec Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1677377517953,
     "user": {
      "displayName": "Shital Virendra Nehete",
      "userId": "02901240228711503219"
     },
     "user_tz": 480
    },
    "id": "SKii2vHZOxi6"
   },
   "outputs": [],
   "source": [
    "# # loading word2vec-google-news-300 Word2Vec Model\n",
    "# pretrained = api.load('word2vec-google-news-300')\n",
    "# pretrained.save('word2vec-google-news.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 45918,
     "status": "ok",
     "timestamp": 1677377563868,
     "user": {
      "displayName": "Shital Virendra Nehete",
      "userId": "02901240228711503219"
     },
     "user_tz": 480
    },
    "id": "uZpE7pmOQVDI"
   },
   "outputs": [],
   "source": [
    "pretrained = gensim.models.KeyedVectors.load('word2vec-google-news.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 418,
     "status": "ok",
     "timestamp": 1677377564455,
     "user": {
      "displayName": "Shital Virendra Nehete",
      "userId": "02901240228711503219"
     },
     "user_tz": 480
    },
    "id": "0szyL-JQHyk1",
    "outputId": "febe0026-04d3-4d12-af12-cf3452ee0be7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "['made', 'hair', 'feel', 'like', 'straw']\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "#Convert Review to a Word List\n",
    "\n",
    "#List to hold all words in each review\n",
    "documents = []\n",
    "\n",
    "#Iterate over each review\n",
    "for doc in df['clean_review_body']:\n",
    "    documents.append(doc.split(' '))\n",
    "\n",
    "print(len(documents))\n",
    "print(documents[0])\n",
    "\n",
    "#Check out review 100 to see how review has been converted into a list of words\n",
    "print(len(documents[100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 60323,
     "status": "ok",
     "timestamp": 1677377624775,
     "user": {
      "displayName": "Shital Virendra Nehete",
      "userId": "02901240228711503219"
     },
     "user_tz": 480
    },
    "id": "Ll2hsSJVIJhZ"
   },
   "outputs": [],
   "source": [
    "#Build the Model\n",
    "\n",
    "model = gensim.models.Word2Vec(documents, #Word list\n",
    "                               min_count=9, #Ignore all words with total frequency lower than this                           \n",
    "                               workers=4, #Number of CPU Cores\n",
    "                               vector_size=300,  #Embedding size\n",
    "                               window=13, #Maximum Distance between current and predicted word\n",
    "                               epochs=10   #Number of iterations over the text corpus\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677377624775,
     "user": {
      "displayName": "Shital Virendra Nehete",
      "userId": "02901240228711503219"
     },
     "user_tz": 480
    },
    "id": "YlAgjsXdIngy",
    "outputId": "ab700f2c-5919-404d-e71a-795bfda0b0f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['made', 'hair', 'feel', 'like', 'straw']\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree ~ Grass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1677377625052,
     "user": {
      "displayName": "Shital Virendra Nehete",
      "userId": "02901240228711503219"
     },
     "user_tz": 480
    },
    "id": "Pj5K_6PjLy1X",
    "outputId": "9f16eaf4-cdae-4ef8-e3bf-892dd2c40c40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For my model, tree and grass have a cosine similarity  0.44956997\n",
      "For pretrained model, tree and grass have a cosine similarity  0.42591316\n"
     ]
    }
   ],
   "source": [
    "print(\"For my model, tree and grass have a cosine similarity \", model.wv.similarity(\"tree\", \"grass\"))\n",
    "print(\"For pretrained model, tree and grass have a cosine similarity \", pretrained.similarity(\"tree\", \"grass\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Road ~ Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For my model, road and track have a cosine similarity  0.31069863\n",
      "For pretrained model, road and track have a cosine similarity  0.3496405\n"
     ]
    }
   ],
   "source": [
    "print(\"For my model, road and track have a cosine similarity \", model.wv.similarity(\"road\", \"track\"))\n",
    "print(\"For pretrained model, road and track have a cosine similarity \", pretrained.similarity(\"road\", \"track\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dog ~ Road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For my model, dog and road have a cosine similarity  0.26113787\n",
      "For pretrained model, dog and road have a cosine similarity  0.082714334\n"
     ]
    }
   ],
   "source": [
    "print(\"For my model, dog and road have a cosine similarity \", model.wv.similarity(\"dog\", \"road\"))\n",
    "print(\"For pretrained model, dog and road have a cosine similarity \", pretrained.similarity(\"dog\", \"road\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blue + Yellow = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For my model, blue + yellow = green, with cosine similarity of 0.8728418946266174\n",
      "For pretrained model, blue + yellow = red, with cosine similarity of 0.8147147297859192\n"
     ]
    }
   ],
   "source": [
    "print(\"For my model, blue + yellow = {}, with cosine similarity of {}\"\n",
    "      .format(model.wv.most_similar(positive=['blue', 'yellow'], topn=1)[0][0], \n",
    "              model.wv.most_similar(positive=['blue', 'yellow'], topn=1)[0][1]))\n",
    "\n",
    "print(\"For pretrained model, blue + yellow = {}, with cosine similarity of {}\"\n",
    "      .format(pretrained.most_similar(positive=['blue', 'yellow'], topn=1)[0][0], \n",
    "              pretrained.most_similar(positive=['blue', 'yellow'], topn=1)[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hs_3gM6Rb34P"
   },
   "source": [
    "### What do you conclude from comparing vectors generated by yourself and the pretrained model?\n",
    "The pretrained model give the similarity for tree and grass as 0.42591316 and my model gives it as 0.44956997.\n",
    "The pretrained model give the similarity for road and track as 0.3496405 and my model gives it as 0.31069863.\n",
    "The pretrained model give the similarity for dog and road as 0.082714334 and my model gives it as 0.26113787.\n",
    "\n",
    "It states that the pretrain model is more accurate than the model trained by me as the similaity scores in higher for the similar words in pretrain model and lower for dissimilar words than my trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeFAJ9OseMK-"
   },
   "source": [
    "### Which of the Word2Vec models seems to encode semantic similarities between words better?\n",
    "The word2vec-google-news-300 Word2Vec model encode the semantic similarities between words better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRpTuBjmezB3"
   },
   "source": [
    "# Task 3: Simple Models (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1677377625215,
     "user": {
      "displayName": "Shital Virendra Nehete",
      "userId": "02901240228711503219"
     },
     "user_tz": 480
    },
    "id": "9zx0zENMUAg9"
   },
   "outputs": [],
   "source": [
    "def printMatrix(matrix):\n",
    "    print(\"Class 1 Precision: \", str(matrix['0']['precision']))\n",
    "    print(\"Class 1 Recall: \", str(matrix['0']['recall']))\n",
    "    print(\"Class 1 f1-score: \", str(matrix['0']['f1-score']))\n",
    "    \n",
    "    print(\"Class 2 Precision: \", str(matrix['1']['precision']))\n",
    "    print(\"Class 2 Recall: \", str(matrix['1']['recall']))\n",
    "    print(\"Class 2 f1-score: \", str(matrix['1']['f1-score']))\n",
    "    \n",
    "    print(\"Class 3 Precison: \", str(matrix['2']['precision']))\n",
    "    print(\"Class 3 Recall: \", str(matrix['2']['recall']))\n",
    "    print(\"Class 3 f1-score: \", str(matrix['2']['f1-score']))\n",
    "    \n",
    "    print(\"Average Precision: \", str(matrix['macro avg']['precision']))\n",
    "    print(\"Average Recall: \", str(matrix['macro avg']['recall']))\n",
    "    print(\"Average f1-score: \", str(matrix['macro avg']['f1-score']))\n",
    "    \n",
    "    print(\"Accuracy: \", str(matrix['accuracy']))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging Word2Vec vectors for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xeLM1li-gLLJ"
   },
   "outputs": [],
   "source": [
    "#adding vectors of all words to a list\n",
    "\n",
    "sentence_vector = []\n",
    "final_sentence_vectors=[]\n",
    "\n",
    "for i in range(len(df)):\n",
    "    row=[]\n",
    "    for j in documents[i]:\n",
    "        if j in pretrained:\n",
    "            row.append(pretrained.get_vector(j))\n",
    "        else:\n",
    "            row.append(np.zeros(300, dtype = int))\n",
    "    sentence_vector.append(row)\n",
    "    \n",
    "    \n",
    "for vector in sentence_vector:\n",
    "    if len(vector)>0:\n",
    "        final_sentence_vectors.append(sum(vector)/len(vector))\n",
    "    else:\n",
    "        final_sentence_vectors.append(np.zeros(300, dtype = int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending to df\n",
    "df['sentence_vector']=final_sentence_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['clean_review_body'])\n",
    "Y = df['rating_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron with tf-idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perceptron Model with tf-idf vectors\n",
    "\n",
    "perceptron = Perceptron(n_jobs = -1, max_iter = 10000, random_state = 10)\n",
    "fit_model = perceptron.fit(X_train,Y_train)\n",
    "Y_pred = perceptron.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 Precision:  0.672300706357215\n",
      "Class 1 Recall:  0.6724703507443855\n",
      "Class 1 f1-score:  0.6723855178503848\n",
      "Class 2 Precision:  0.5497076023391813\n",
      "Class 2 Recall:  0.5854509217737918\n",
      "Class 2 f1-score:  0.5670165279285799\n",
      "Class 3 Precison:  0.7434193033767615\n",
      "Class 3 Recall:  0.6950037285607755\n",
      "Class 3 f1-score:  0.7183967112024666\n",
      "Average Precision:  0.6551425373577192\n",
      "Average Recall:  0.650975000359651\n",
      "Average f1-score:  0.652599585660477\n",
      "Accuracy:  0.6509166666666667\n"
     ]
    }
   ],
   "source": [
    "printMatrix(metrics.classification_report(Y_test, Y_pred, output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with tf-idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM Model with tf-idf vectors\n",
    "\n",
    "svmClassifier = svm.LinearSVC(C = 0.01, multi_class=\"ovr\", random_state = 10)\n",
    "svmClassifier.fit(X_train, Y_train)\n",
    "Y_pred = svmClassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 Precision:  0.7023060796645703\n",
      "Class 1 Recall:  0.7607872823618471\n",
      "Class 1 f1-score:  0.7303779069767441\n",
      "Class 2 Precision:  0.6462547840349918\n",
      "Class 2 Recall:  0.5889387144992526\n",
      "Class 2 f1-score:  0.6162669447340982\n",
      "Class 3 Precison:  0.7829093603358854\n",
      "Class 3 Recall:  0.7879691772309222\n",
      "Class 3 f1-score:  0.7854311199207137\n",
      "Average Precision:  0.7104900746784825\n",
      "Average Recall:  0.712565058030674\n",
      "Average f1-score:  0.710691990543852\n",
      "Accuracy:  0.7124166666666667\n"
     ]
    }
   ],
   "source": [
    "printMatrix(metrics.classification_report(Y_test, Y_pred, output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=final_sentence_vectors\n",
    "Y=df['rating_class']\n",
    "\n",
    "#splitting train and test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron with word2vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perceptron Model with Word2Vec vectors\n",
    "\n",
    "perceptron = Perceptron(n_jobs = -1, max_iter = 10000, random_state = 10)\n",
    "fit_model = perceptron.fit(X_train,Y_train)\n",
    "Y_pred = perceptron.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 Precision:  0.7261549925484352\n",
      "Class 1 Recall:  0.49179914206409286\n",
      "Class 1 f1-score:  0.5864299684068001\n",
      "Class 2 Precision:  0.6183013144590496\n",
      "Class 2 Recall:  0.3046836073741903\n",
      "Class 2 f1-score:  0.4082109479305741\n",
      "Class 3 Precison:  0.49618424638866176\n",
      "Class 3 Recall:  0.9050459855828983\n",
      "Class 3 f1-score:  0.6409647038112841\n",
      "Average Precision:  0.6135468511320489\n",
      "Average Recall:  0.5671762450070604\n",
      "Average f1-score:  0.5452018733828861\n",
      "Accuracy:  0.56775\n"
     ]
    }
   ],
   "source": [
    "printMatrix(metrics.classification_report(Y_test, Y_pred, output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with word2vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM Model with Word2Vec vectors\n",
    "\n",
    "svmClassifier = svm.LinearSVC(C = 0.01, multi_class=\"ovr\", random_state = 10)\n",
    "svmClassifier.fit(X_train, Y_train)\n",
    "Y_pred = svmClassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 Precision:  0.6486976217440543\n",
      "Class 1 Recall:  0.7226848347211708\n",
      "Class 1 f1-score:  0.6836953926951539\n",
      "Class 2 Precision:  0.5993413830954994\n",
      "Class 2 Recall:  0.5440956651718983\n",
      "Class 2 f1-score:  0.5703839122486289\n",
      "Class 3 Precison:  0.7480334940370464\n",
      "Class 3 Recall:  0.7327864777529207\n",
      "Class 3 f1-score:  0.7403314917127073\n",
      "Average Precision:  0.6653574996255335\n",
      "Average Recall:  0.6665223258819967\n",
      "Average f1-score:  0.6648035988854967\n",
      "Accuracy:  0.6663333333333333\n"
     ]
    }
   ],
   "source": [
    "printMatrix(metrics.classification_report(Y_test, Y_pred, output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained Word2Vec features)?\n",
    "\n",
    "Tf-idf vectorization outstands the performance of Word2Vec in Simple models like Perceptron and SVM. As accuracy for Perceptron is 0.6509166666666667 and 0.56775 for tf-idf and word2vec respectively and accuracy for SVM is 0.7124166666666667 and 0.6663333333333333 for tf-idf and word2vec respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Feedforward Neural Networks (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data and converting them to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=final_sentence_vectors\n",
    "Y=df['rating_class']\n",
    "\n",
    "# Load your data and split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Convert label data to tensor\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Remove any None values\n",
    "X_train_vectors = [vector for vector in X_train if vector is not None]\n",
    "\n",
    "# Remove any None values\n",
    "X_test_vectors = [vector for vector in X_test if vector is not None]\n",
    "\n",
    "# Convert label data to tensor\n",
    "y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "y_test = torch.tensor(y_test, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the MLP model considering the average Word2Vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_j/sb0mgtc11kg8cclmrq736qbr0000gn/T/ipykernel_13664/210265164.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  accuracy = torch.mean((y_pred == torch.tensor(y_test)).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Accuracy: 0.6672499775886536\n",
      "Epoch 2 Accuracy: 0.6747499704360962\n",
      "Epoch 3 Accuracy: 0.6785833239555359\n",
      "Epoch 4 Accuracy: 0.6814166903495789\n",
      "Epoch 5 Accuracy: 0.6831666827201843\n",
      "Epoch 6 Accuracy: 0.6827499866485596\n",
      "Epoch 7 Accuracy: 0.6839166879653931\n",
      "Epoch 8 Accuracy: 0.6849166750907898\n",
      "Epoch 9 Accuracy: 0.6864166855812073\n",
      "Epoch 10 Accuracy: 0.6863333582878113\n",
      "Epoch 11 Accuracy: 0.6855000257492065\n",
      "Epoch 12 Accuracy: 0.6865833401679993\n",
      "Epoch 13 Accuracy: 0.6852499842643738\n",
      "Epoch 14 Accuracy: 0.6848333477973938\n",
      "Epoch 15 Accuracy: 0.6859166622161865\n",
      "Epoch 16 Accuracy: 0.684166669845581\n",
      "Epoch 17 Accuracy: 0.6830833554267883\n",
      "Epoch 18 Accuracy: 0.6835833191871643\n",
      "Epoch 19 Accuracy: 0.6827499866485596\n",
      "Epoch 20 Accuracy: 0.6817499995231628\n"
     ]
    }
   ],
   "source": [
    "# Define training parameters\n",
    "input_size = X_train_vectors[0].shape[0]\n",
    "hidden_size1 = 100\n",
    "hidden_size2 = 10\n",
    "output_size = len(np.unique(y_train))\n",
    "lr = 0.001\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "# Define MLP model\n",
    "mlp_model = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=lr)\n",
    "\n",
    "# Train MLP model\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(X_train_vectors), batch_size):\n",
    "        batch_X = torch.stack(X_train_vectors[i:i+batch_size])\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate MLP model on testing data after each epoch\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.stack(X_test_vectors)\n",
    "        y_pred = torch.argmax(mlp_model(X_test_tensor), dim=1)\n",
    "        accuracy = torch.mean((y_pred == torch.tensor(y_test)).float())\n",
    "        print(f'Epoch {epoch+1} Accuracy: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating first 10 Word2Vec vectors for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#considering first 10 words for each row\n",
    "\n",
    "first_10_vectors=[]\n",
    "\n",
    "for i in range(len(df)):\n",
    "    row=[]\n",
    "    for j in range(10):\n",
    "        if j<len(documents[i]):\n",
    "            if documents[i][j] in pretrained:\n",
    "                row.extend(pretrained.get_vector(documents[i][j]))\n",
    "            else:\n",
    "                row.extend(np.zeros(300, dtype = float))\n",
    "        else:\n",
    "            row.extend(np.zeros(300, dtype = float))\n",
    "    first_10_vectors.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data and converting them to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=first_10_vectors\n",
    "Y=df['rating_class']\n",
    "\n",
    "# Load your data and split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Convert label data to tensor\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Remove any None values\n",
    "X_train_vectors = [vector for vector in X_train if vector is not None]\n",
    "\n",
    "# Remove any None values\n",
    "X_test_vectors = [vector for vector in X_test if vector is not None]\n",
    "\n",
    "# Convert label data to tensor\n",
    "y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "y_test = torch.tensor(y_test, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the MLP model considering the first 10 concatenated Word2Vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_j/sb0mgtc11kg8cclmrq736qbr0000gn/T/ipykernel_13664/3434361302.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  accuracy = torch.mean((y_pred == torch.tensor(y_test)).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Accuracy: 0.6261666417121887\n",
      "Epoch 2 Accuracy: 0.6290833353996277\n"
     ]
    }
   ],
   "source": [
    "# Define training parameters\n",
    "input_size = X_train_vectors[0].shape[0]\n",
    "hidden_size1 = 100\n",
    "hidden_size2 = 10\n",
    "output_size = len(np.unique(y_train))\n",
    "lr = 0.001\n",
    "epochs = 2\n",
    "# epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Define MLP model\n",
    "mlp_model = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=lr)\n",
    "\n",
    "# Train MLP model\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(X_train_vectors), batch_size):\n",
    "        batch_X = torch.stack(X_train_vectors[i:i+batch_size])\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate MLP model on testing data after each epoch\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.stack(X_test_vectors)\n",
    "        y_pred = torch.argmax(mlp_model(X_test_tensor), dim=1)\n",
    "        accuracy = torch.mean((y_pred == torch.tensor(y_test)).float())\n",
    "        print(f'Epoch {epoch+1} Accuracy: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section.\n",
    "\n",
    "The accuracy of Simple Models are 0.56775 and 0.6663333333333333 for perceptron and svm respectively. Here, the MLP gives accuracy 0.6817499995231628 and 0.6290833353996277 with Word2Vec vectors performing slightly better than simple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Recurrent Neural Networks (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating word2vec vectors with the maximum limit of 20 words and padding the shorter reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding vectors of all words to a list\n",
    "\n",
    "rnn_sentence_vectors_combined = []\n",
    "final_rnn_sentence_vectors=[]\n",
    "\n",
    "for i in range(len(df)):\n",
    "    row=[]\n",
    "    for j in range(20):\n",
    "        if j<len(documents[i]):\n",
    "            if documents[i][j] in pretrained:\n",
    "                row.append(pretrained.get_vector(documents[i][j]))\n",
    "            else:\n",
    "                row.append(np.zeros(300, dtype = float))\n",
    "        else:\n",
    "            row.append(np.zeros(300, dtype = float))\n",
    "    rnn_sentence_vectors_combined.append(row)\n",
    "    \n",
    "    \n",
    "#averaging the vectors for getting single 300 embedding list for each row\n",
    "\n",
    "for vector in rnn_sentence_vectors_combined:\n",
    "    if len(vector)>0:\n",
    "        final_rnn_sentence_vectors.append(sum(vector)/len(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rnn_sentence_vectors_combined[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_rnn_sentence_vectors[3781])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data and converting to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=final_rnn_sentence_vectors\n",
    "Y=df['rating_class']\n",
    "\n",
    "# Load your data and split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN Model with hidden state size of 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnnModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(rnnModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.layer = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_dim**2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.layer(x, hidden)\n",
    "        out = out.contiguous().view(-1, out.shape[1] * out.shape[2])\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    target = [item[1] for item in batch]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Data(Dataset):\n",
    "    def __init__(self, X_data, Y_data):\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "    def __getitem__(self, index):\n",
    "        pad = np.zeros((20, 300), dtype = float)\n",
    "        pad[-len(self.X_data[index]):] = np.array(self.X_data[index])\n",
    "        X = torch.FloatTensor(pad)\n",
    "        Y = torch.tensor(self.Y_data[index])\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = rnnModel(300, 3, 20, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Simple RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_j/sb0mgtc11kg8cclmrq736qbr0000gn/T/ipykernel_13664/929018065.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.014011\n",
      "Epoch: 2 \tTraining Loss: 1.079742\n",
      "Epoch: 3 \tTraining Loss: 0.490197\n",
      "Epoch: 4 \tTraining Loss: 0.799531\n",
      "Epoch: 5 \tTraining Loss: 0.727060\n",
      "Accuracy for RNN model: 0.6365833333333333\n"
     ]
    }
   ],
   "source": [
    "rnn_train = RNN_Data(X_train, y_train)\n",
    "train_loader_mode = DataLoader(dataset = rnn_train, batch_size=8, shuffle = True, collate_fn=my_collate, drop_last=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.0001)\n",
    "epoch=5\n",
    "\n",
    "for ep in range(1, epoch + 1):\n",
    "    for input_data, label in train_loader_mode:\n",
    "        optimizer.zero_grad()\n",
    "        input_data = torch.stack(input_data)\n",
    "        label = torch.stack(label)\n",
    "        output, hidden = rnn(input_data)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        loss = criterion(output,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(ep, loss.item()))\n",
    "    \n",
    "rnn_test = RNN_Data(X_test, y_test)\n",
    "test_loader_mode = DataLoader(dataset = rnn_test, batch_size=8, collate_fn=my_collate, drop_last=True)\n",
    "\n",
    "predictions, actual = list(), list()\n",
    "for test_data, test_label in test_loader_mode:\n",
    "    test_data = torch.stack(test_data)\n",
    "    test_label = torch.stack(test_label)\n",
    "    pred, hid = rnn(test_data.to('cpu'))\n",
    "    pred = pred.to('cpu')\n",
    "    pred = pred.detach().numpy()\n",
    "    pred = argmax(pred, axis= 1)\n",
    "    target = test_label.numpy()\n",
    "    target = target.reshape((len(target), 1))\n",
    "    pred = pred.reshape((len(pred)), 1)\n",
    "    pred = pred.round()\n",
    "    predictions.append(pred)\n",
    "    actual.append(target)\n",
    "\n",
    "predictions, actual = vstack(predictions), vstack(actual)\n",
    "acc = accuracy_score(actual, predictions)\n",
    "print('Accuracy for RNN model: ' + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models.\n",
    "The accuracy for RNN model considering sentence vectors for first 20 words and padding the shorter sentences is 0.6404166666666666. The FNN model considering sentence vectors of whole sentence is 0.6823333501815796 and while considering combined vectors of first 10 words is 0.6328333616256714. So, the accuracy is better with FNN model than RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit (GRU) model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gruModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(gruModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.layer = nn.GRU(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_dim**2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.layer(x, hidden)\n",
    "        out = out.contiguous().view(-1, out.shape[1] * out.shape[2])\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = gruModel(300, 3, 20, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_j/sb0mgtc11kg8cclmrq736qbr0000gn/T/ipykernel_13664/1582279110.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.802263\n",
      "Epoch: 2 \tTraining Loss: 0.731870\n",
      "Epoch: 3 \tTraining Loss: 0.516542\n",
      "Epoch: 4 \tTraining Loss: 0.897207\n",
      "Epoch: 5 \tTraining Loss: 0.658499\n",
      "Accuracy for GRU model: 0.6433333333333333\n"
     ]
    }
   ],
   "source": [
    "gru_train = RNN_Data(X_train, y_train)\n",
    "train_loader_mode = DataLoader(dataset = gru_train, batch_size=8, shuffle = True, collate_fn=my_collate, drop_last=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gru.parameters(), lr=0.0001)\n",
    "epoch=5\n",
    "\n",
    "for ep in range(1, epoch + 1):\n",
    "    for input_data, label in train_loader_mode:\n",
    "        optimizer.zero_grad()\n",
    "        input_data = torch.stack(input_data)\n",
    "        label = torch.stack(label)\n",
    "        output, hidden = gru(input_data)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        loss = criterion(output,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(ep, loss.item()))\n",
    "    \n",
    "gru_test = RNN_Data(X_test, y_test)\n",
    "test_loader_mode = DataLoader(dataset = gru_test, batch_size=8, collate_fn=my_collate, drop_last=True)\n",
    "\n",
    "predictions, actual = list(), list()\n",
    "for test_data, test_label in test_loader_mode:\n",
    "    test_data = torch.stack(test_data)\n",
    "    test_label = torch.stack(test_label)\n",
    "    pred, hid = gru(test_data.to('cpu'))\n",
    "    pred = pred.to('cpu')\n",
    "    pred = pred.detach().numpy()\n",
    "    pred = argmax(pred, axis= 1)\n",
    "    target = test_label.numpy()\n",
    "    target = target.reshape((len(target), 1))\n",
    "    pred = pred.reshape((len(pred)), 1)\n",
    "    pred = pred.round()\n",
    "    predictions.append(pred)\n",
    "    actual.append(target)\n",
    "\n",
    "predictions, actual = vstack(predictions), vstack(actual)\n",
    "acc = accuracy_score(actual, predictions)\n",
    "print('Accuracy for GRU model: ' + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstmModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(lstmModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.layer = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_dim**2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.layer(x, hidden)\n",
    "        out = out.contiguous().view(-1, out.shape[1] * out.shape[2])\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_dim), \n",
    "                  torch.zeros(self.n_layers, batch_size, self.hidden_dim))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = lstmModel(300, 3, 20, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_j/sb0mgtc11kg8cclmrq736qbr0000gn/T/ipykernel_13664/3571706937.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.190580\n",
      "Epoch: 2 \tTraining Loss: 0.913236\n",
      "Epoch: 3 \tTraining Loss: 0.609997\n",
      "Epoch: 4 \tTraining Loss: 0.980714\n",
      "Epoch: 5 \tTraining Loss: 1.073116\n",
      "Accuracy for LSTM model: 0.6438333333333334\n"
     ]
    }
   ],
   "source": [
    "lstm_train = RNN_Data(X_train, y_train)\n",
    "train_loader_mode = DataLoader(dataset = lstm_train, batch_size=8, shuffle = True, collate_fn=my_collate, drop_last=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.0001)\n",
    "epoch=5\n",
    "\n",
    "for ep in range(1, epoch + 1):\n",
    "    for input_data, label in train_loader_mode:\n",
    "        optimizer.zero_grad()\n",
    "        input_data = torch.stack(input_data)\n",
    "        label = torch.stack(label)\n",
    "        output, hidden = lstm(input_data)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        loss = criterion(output,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(ep, loss.item()))\n",
    "    \n",
    "lstm_test = RNN_Data(X_test, y_test)\n",
    "test_loader_mode = DataLoader(dataset = lstm_test, batch_size=8, collate_fn=my_collate, drop_last=True)\n",
    "\n",
    "predictions, actual = list(), list()\n",
    "for test_data, test_label in test_loader_mode:\n",
    "    test_data = torch.stack(test_data)\n",
    "    test_label = torch.stack(test_label)\n",
    "    pred, hid = lstm(test_data.to('cpu'))\n",
    "    pred = pred.to('cpu')\n",
    "    pred = pred.detach().numpy()\n",
    "    pred = argmax(pred, axis= 1)\n",
    "    target = test_label.numpy()\n",
    "    target = target.reshape((len(target), 1))\n",
    "    pred = pred.reshape((len(pred)), 1)\n",
    "    pred = pred.round()\n",
    "    predictions.append(pred)\n",
    "    actual.append(target)\n",
    "\n",
    "predictions, actual = vstack(predictions), vstack(actual)\n",
    "acc = accuracy_score(actual, predictions)\n",
    "print('Accuracy for LSTM model: ' + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you conclude by comparing accuracy values you obtain by GRU, LSTM, and simple RNN.\n",
    "The accuracy are 0.6365833333333333, 0.6433333333333333, 0.6438333333333334 for simple RNN, GRU and LSTM respectively. The accuracy for LSTM is slightly higher than the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOWW5YrbhZhNoWJH66L2dG3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
